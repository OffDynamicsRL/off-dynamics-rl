# References

ODRL is motivated and built on top of some commonly adopted environments and benchmarks, including:

- [OpenAI Gym](https://arxiv.org/pdf/1606.01540.pdf)
- [D4RL](https://arxiv.org/pdf/2004.07219)
- [Meta-World](http://proceedings.mlr.press/v100/yu20a/yu20a.pdf)

ODRL is also motivated by the following off-dynamics RL papers. We highly recommend the users to read these papers:

- [Off-dynamics reinforcement learning: Training for transfer with domain classifiers](https://arxiv.org/pdf/2006.13916)
- [Cross-domain policy adaptation via value-guided data filtering](https://proceedings.neurips.cc/paper_files/paper/2023/file/e8ad87f1076fb0f75d89a45828f186b0-Paper-Conference.pdf)
- [Cross-domain policy adaptation by capturing representation mismatch](https://arxiv.org/pdf/2405.15369)
- [When to trust your simulator: Dynamics-aware hybrid offline-and-online reinforcement learning](https://proceedings.neurips.cc/paper_files/paper/2022/file/ed3cd2520148b577039adfade82a5566-Paper-Conference.pdf)
- [Dara: Dynamics-aware reward augmentation in offline reinforcement learning](https://arxiv.org/pdf/2203.06662)